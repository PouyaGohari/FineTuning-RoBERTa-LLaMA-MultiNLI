{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft\n!pip install datasets\n!pip install bitsandbytes\n!pip install accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-01T21:09:57.101792Z","iopub.execute_input":"2024-06-01T21:09:57.102152Z","iopub.status.idle":"2024-06-01T21:10:41.944350Z","shell.execute_reply.started":"2024-06-01T21:09:57.102120Z","shell.execute_reply":"2024-06-01T21:10:41.943403Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\n\u001b[0m\u001b[31m\n\u001b[0mCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, PreTrainedModel, LlamaPreTrainedModel, LlamaModel, AutoConfig\n\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom datasets import load_dataset\nfrom huggingface_hub import notebook_login\nimport torch\nfrom transformers.modeling_outputs import SequenceClassifierOutputWithPast","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:10:41.946309Z","iopub.execute_input":"2024-06-01T21:10:41.946639Z","iopub.status.idle":"2024-06-01T21:10:49.199988Z","shell.execute_reply.started":"2024-06-01T21:10:41.946610Z","shell.execute_reply":"2024-06-01T21:10:49.199232Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:10:49.201648Z","iopub.execute_input":"2024-06-01T21:10:49.202122Z","iopub.status.idle":"2024-06-01T21:10:49.209177Z","shell.execute_reply.started":"2024-06-01T21:10:49.202093Z","shell.execute_reply":"2024-06-01T21:10:49.208243Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ndataset_name = \"multi_nli\"","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:10:49.210375Z","iopub.execute_input":"2024-06-01T21:10:49.210733Z","iopub.status.idle":"2024-06-01T21:10:49.218937Z","shell.execute_reply.started":"2024-06-01T21:10:49.210706Z","shell.execute_reply":"2024-06-01T21:10:49.218061Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"notebook_login()\n#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","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:10:49.220566Z","iopub.execute_input":"2024-06-01T21:10:49.220858Z","iopub.status.idle":"2024-06-01T21:10:49.244871Z","shell.execute_reply.started":"2024-06-01T21:10:49.220824Z","shell.execute_reply":"2024-06-01T21:10:49.243912Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"002c4fe1791f4b5baff0cd934c971946"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:10:57.638898Z","iopub.execute_input":"2024-06-01T21:10:57.639278Z","iopub.status.idle":"2024-06-01T21:10:58.729854Z","shell.execute_reply.started":"2024-06-01T21:10:57.639248Z","shell.execute_reply":"2024-06-01T21:10:58.728800Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e1432c11fb4c1aa469f03e62599d77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"857d3846e6904cf5b0ed9df1d20e8c66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"339bc3e9337d44b1b08c45501cd5d74b"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport bitsandbytes as bnb\nfrom torch.nn import CrossEntropyLoss\nclass CLF(LlamaPreTrainedModel):\n    def __init__(self, config, num_labels):\n        super().__init__(config)\n        self.num_labels = num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n        self.post_init()\n    \n    def forward(self, input_ids=None,\n                attention_mask=None,\n                labels=None,\n                position_ids=None,\n                past_key_values=None,\n                inputs_embeds=None,\n                use_cache=None,\n                output_attentions=None,\n                output_hidden_states=None,\n                return_dict=None):        \n        \n        transformers_outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict)\n\n        hidden_states = transformers_outputs.last_hidden_state\n        logits = self.score(hidden_states)\n        \n        batch_size = input_ids.shape[0]\n        \n        sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n        sequence_lengths = sequence_lengths % input_ids.shape[-1]\n        sequence_lengths = sequence_lengths.to(logits.device)\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n        \n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n        \n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n        \n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformers_outputs.past_key_values,\n            hidden_states=transformers_outputs.hidden_states,\n            attentions=transformers_outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:10:58.944336Z","iopub.execute_input":"2024-06-01T21:10:58.945091Z","iopub.status.idle":"2024-06-01T21:10:59.001037Z","shell.execute_reply.started":"2024-06-01T21:10:58.945036Z","shell.execute_reply":"2024-06-01T21:10:59.000293Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.float16,\n    bnb_4bit_use_double_quant= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:10:59.797405Z","iopub.execute_input":"2024-06-01T21:10:59.798000Z","iopub.status.idle":"2024-06-01T21:10:59.960575Z","shell.execute_reply.started":"2024-06-01T21:10:59.797970Z","shell.execute_reply":"2024-06-01T21:10:59.959452Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"num_labels = 3\nid2label = {0: 'entailment', 1: 'contradiction', 2: 'neutral'}\nlabel2id = {label: idx for idx, label in id2label.items()}","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:11:00.355426Z","iopub.execute_input":"2024-06-01T21:11:00.355816Z","iopub.status.idle":"2024-06-01T21:11:00.360713Z","shell.execute_reply.started":"2024-06-01T21:11:00.355782Z","shell.execute_reply":"2024-06-01T21:11:00.359762Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_name, num_labels=len(id2label))\nconfig.id2label = id2label\nconfig.label2id = label2id\nconfig.pad_token_id = tokenizer.pad_token_id\nmodel = CLF.from_pretrained(\n    model_name,\n    config=config,\n    num_labels=len(id2label),\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"eager\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:11:01.304515Z","iopub.execute_input":"2024-06-01T21:11:01.305192Z","iopub.status.idle":"2024-06-01T21:13:30.749416Z","shell.execute_reply.started":"2024-06-01T21:11:01.305159Z","shell.execute_reply":"2024-06-01T21:13:30.748400Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f647bfa62e9940fc8d9735547486a91a"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b003c63fd14e21a283c205bc239c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dd980c8a959453ead417b26780b7cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"215c8972fb1e4561a037d70ecf7a87f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b3a2dc23f34b47ab504cc7c16108fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e07190a67c543f08176edcbfc143434"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b105e53932ab4b9ea2f5590db71ad3b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e8839233fd4068b9eec9149ae3122b"}},"metadata":{}},{"name":"stderr","text":"Some weights of CLF were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:33.986195Z","iopub.execute_input":"2024-06-01T21:13:33.987214Z","iopub.status.idle":"2024-06-01T21:13:34.016228Z","shell.execute_reply.started":"2024-06-01T21:13:33.987162Z","shell.execute_reply":"2024-06-01T21:13:34.015454Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:34.278799Z","iopub.execute_input":"2024-06-01T21:13:34.279136Z","iopub.status.idle":"2024-06-01T21:13:34.286469Z","shell.execute_reply.started":"2024-06-01T21:13:34.279109Z","shell.execute_reply":"2024-06-01T21:13:34.285541Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CLF(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (score): Linear(in_features=4096, out_features=3, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha= 16,\n    lora_dropout= 0.05,\n    r= 16,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\", \"score\"]\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:34.672405Z","iopub.execute_input":"2024-06-01T21:13:34.672729Z","iopub.status.idle":"2024-06-01T21:13:34.678016Z","shell.execute_reply.started":"2024-06-01T21:13:34.672705Z","shell.execute_reply":"2024-06-01T21:13:34.676904Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:35.086628Z","iopub.execute_input":"2024-06-01T21:13:35.087423Z","iopub.status.idle":"2024-06-01T21:13:35.983197Z","shell.execute_reply.started":"2024-06-01T21:13:35.087390Z","shell.execute_reply":"2024-06-01T21:13:35.982261Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"trainable params: 42,020,912 || all params: 7,547,023,456 || trainable%: 0.5568\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset(dataset_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:35.984944Z","iopub.execute_input":"2024-06-01T21:13:35.985612Z","iopub.status.idle":"2024-06-01T21:13:43.515927Z","shell.execute_reply.started":"2024-06-01T21:13:35.985577Z","shell.execute_reply":"2024-06-01T21:13:43.515229Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9847e7a891b9494d8b96574c327ae21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f551b780f8a45e2bd041e1be87111d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb6a2e7329a49179e4ead34cd1543d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3f2938e00514f53826493be7fa09730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca10d5f71d3b46359734ff6b4d375b0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f987b24a2a644e8aab72fdb215e9d333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09eec23ec2c34cafbe50b80282c2dae3"}},"metadata":{}}]},{"cell_type":"code","source":"model.get_memory_footprint()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:44.965113Z","iopub.execute_input":"2024-06-01T21:13:44.966068Z","iopub.status.idle":"2024-06-01T21:13:44.989352Z","shell.execute_reply.started":"2024-06-01T21:13:44.966029Z","shell.execute_reply":"2024-06-01T21:13:44.987955Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"5760475520"},"metadata":{}}]},{"cell_type":"code","source":"import random\ndef sample_5_percent(dataset):\n    total_size = len(dataset)\n    sample_size = max(1, 5 * total_size // 100)\n    indices = random.sample(range(total_size), sample_size)\n    return dataset.select(indices)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:46.026584Z","iopub.execute_input":"2024-06-01T21:13:46.026932Z","iopub.status.idle":"2024-06-01T21:13:46.032197Z","shell.execute_reply.started":"2024-06-01T21:13:46.026905Z","shell.execute_reply":"2024-06-01T21:13:46.031309Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sampled_train_dataset = sample_5_percent(dataset['train'])\nsampled_validation_dataset = sample_5_percent(dataset['validation_matched'])\nsampled_train_dataset.shape, sampled_validation_dataset.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:47.036171Z","iopub.execute_input":"2024-06-01T21:13:47.036540Z","iopub.status.idle":"2024-06-01T21:13:47.165001Z","shell.execute_reply.started":"2024-06-01T21:13:47.036511Z","shell.execute_reply":"2024-06-01T21:13:47.164169Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"((19635, 10), (490, 10))"},"metadata":{}}]},{"cell_type":"code","source":"INSTRUCTION = \"Decide whether the following statements are entailment, contradiction, or neutral.\"","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:48.166489Z","iopub.execute_input":"2024-06-01T21:13:48.166847Z","iopub.status.idle":"2024-06-01T21:13:48.171013Z","shell.execute_reply.started":"2024-06-01T21:13:48.166817Z","shell.execute_reply":"2024-06-01T21:13:48.170085Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def create_prompt(example, instruction):\n    premise = example['premise']\n    hypothesis = example['hypothesis']\n    user_input = f\"\\nPremise: {premise}\\nHypothesis: {hypothesis}\\n\"\n    prompt = f\"\"\"<|start_header_id|>system<|end_header_id|> {instruction}<|eot_id|><|start_header_id|>user<|end_header_id|> Are these two statements entailment, contradiction, or neutral?: {user_input}<|eot_id|>\"\"\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:49.081289Z","iopub.execute_input":"2024-06-01T21:13:49.082246Z","iopub.status.idle":"2024-06-01T21:13:49.088168Z","shell.execute_reply.started":"2024-06-01T21:13:49.082204Z","shell.execute_reply":"2024-06-01T21:13:49.087100Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_dataset = sampled_train_dataset.map(lambda example: {'prompt': create_prompt(example, INSTRUCTION)})","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:49.727970Z","iopub.execute_input":"2024-06-01T21:13:49.728345Z","iopub.status.idle":"2024-06-01T21:13:54.168255Z","shell.execute_reply.started":"2024-06-01T21:13:49.728318Z","shell.execute_reply":"2024-06-01T21:13:54.167303Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19635 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ff80e2fe3a14fc9893299d880df0685"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset['prompt'][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:56.248360Z","iopub.execute_input":"2024-06-01T21:13:56.249257Z","iopub.status.idle":"2024-06-01T21:13:56.293154Z","shell.execute_reply.started":"2024-06-01T21:13:56.249221Z","shell.execute_reply":"2024-06-01T21:13:56.292191Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"\"<|start_header_id|>system<|end_header_id|> Decide whether the following statements are entailment, contradiction, or neutral.<|eot_id|><|start_header_id|>user<|end_header_id|> Are these two statements entailment, contradiction, or neutral?: \\nPremise: What's more, they observe something called the minimal group effect.\\nHypothesis: The minimal group effect is also observed.\\n<|eot_id|>\""},"metadata":{}}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:56.911241Z","iopub.execute_input":"2024-06-01T21:13:56.912181Z","iopub.status.idle":"2024-06-01T21:13:56.917836Z","shell.execute_reply.started":"2024-06-01T21:13:56.912149Z","shell.execute_reply":"2024-06-01T21:13:56.916796Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label', 'prompt'],\n    num_rows: 19635\n})"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    outputs = tokenizer(examples['prompt'], truncation=True, max_length=None)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:57.688613Z","iopub.execute_input":"2024-06-01T21:13:57.689298Z","iopub.status.idle":"2024-06-01T21:13:57.693733Z","shell.execute_reply.started":"2024-06-01T21:13:57.689263Z","shell.execute_reply":"2024-06-01T21:13:57.692673Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_tokenized_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"promptID\", \"pairID\", \"premise\", \"premise_binary_parse\", \"premise_parse\", \"hypothesis\", \"hypothesis_binary_parse\", \"hypothesis_parse\", \"genre\", \"prompt\"]).rename_column('label', 'labels')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:13:58.273763Z","iopub.execute_input":"2024-06-01T21:13:58.274601Z","iopub.status.idle":"2024-06-01T21:14:00.984749Z","shell.execute_reply.started":"2024-06-01T21:13:58.274569Z","shell.execute_reply":"2024-06-01T21:14:00.983832Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19635 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0025eff52d334bbf87b01df47c7310b4"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:14:00.986540Z","iopub.execute_input":"2024-06-01T21:14:00.987120Z","iopub.status.idle":"2024-06-01T21:14:00.993272Z","shell.execute_reply.started":"2024-06-01T21:14:00.987084Z","shell.execute_reply":"2024-06-01T21:14:00.992381Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 19635\n})"},"metadata":{}}]},{"cell_type":"code","source":"class CustomDataCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, features):\n        batch = self.tokenizer.pad(\n            features,\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:14:03.959943Z","iopub.execute_input":"2024-06-01T21:14:03.960330Z","iopub.status.idle":"2024-06-01T21:14:03.966303Z","shell.execute_reply.started":"2024-06-01T21:14:03.960302Z","shell.execute_reply":"2024-06-01T21:14:03.965310Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"data_collator = CustomDataCollator(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:14:04.807320Z","iopub.execute_input":"2024-06-01T21:14:04.807678Z","iopub.status.idle":"2024-06-01T21:14:04.811952Z","shell.execute_reply.started":"2024-06-01T21:14:04.807650Z","shell.execute_reply":"2024-06-01T21:14:04.810975Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom accelerate import Accelerator\n\ntraining_args=TrainingArguments(\n        # num_train_epochs= 1,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        logging_steps= 30,\n        warmup_steps=5,\n        max_steps=240,\n        learning_rate=2e-5,\n        weight_decay= 0.01,\n        fp16= False,\n        bf16= False,\n        max_grad_norm= 0.3,\n        group_by_length= True,\n        optim=\"paged_adamw_8bit\",\n        output_dir= \"/content/drive/MyDrive/weigths\",\n        report_to=\"none\", \n)\n\n# accelerator = Accelerator()\n# model_with_clf, tokenizer, training_args = accelerator.prepare(model_with_clf, tokenizer, training_args)\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_tokenized_dataset,\n    args=training_args,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:14:05.408927Z","iopub.execute_input":"2024-06-01T21:14:05.409685Z","iopub.status.idle":"2024-06-01T21:14:17.097032Z","shell.execute_reply.started":"2024-06-01T21:14:05.409652Z","shell.execute_reply":"2024-06-01T21:14:17.096061Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"2024-06-01 21:14:07.753144: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-01 21:14:07.753253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-01 21:14:07.905701: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"model.train()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T21:14:17.098779Z","iopub.execute_input":"2024-06-01T21:14:17.099423Z","iopub.status.idle":"2024-06-01T22:04:23.904551Z","shell.execute_reply.started":"2024-06-01T21:14:17.099394Z","shell.execute_reply":"2024-06-01T22:04:23.903522Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [240/240 49:36, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>30</td>\n      <td>1.238900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.126600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.171900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.121700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.123400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.114600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.122800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.100900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=240, training_loss=1.1401063760121664, metrics={'train_runtime': 3003.4363, 'train_samples_per_second': 1.279, 'train_steps_per_second': 0.08, 'total_flos': 1.31816723460864e+16, 'train_loss': 1.1401063760121664, 'epoch': 0.1955591770217967})"},"metadata":{}}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:05:43.844831Z","iopub.execute_input":"2024-06-01T22:05:43.845509Z","iopub.status.idle":"2024-06-01T22:05:43.884619Z","shell.execute_reply.started":"2024-06-01T22:05:43.845475Z","shell.execute_reply":"2024-06-01T22:05:43.883467Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): CLF(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): lora.Linear(\n          (base_layer): Linear(in_features=4096, out_features=3, bias=False)\n          (lora_dropout): ModuleDict(\n            (default): Dropout(p=0.05, inplace=False)\n          )\n          (lora_A): ModuleDict(\n            (default): Linear(in_features=4096, out_features=16, bias=False)\n          )\n          (lora_B): ModuleDict(\n            (default): Linear(in_features=16, out_features=3, bias=False)\n          )\n          (lora_embedding_A): ParameterDict()\n          (lora_embedding_B): ParameterDict()\n        )\n        (modules_to_save): ModuleDict(\n          (default): lora.Linear(\n            (base_layer): Linear(in_features=4096, out_features=3, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=3, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"validation_set = sampled_validation_dataset.map(lambda example: {'prompt': create_prompt(example, INSTRUCTION)})","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:05:44.627094Z","iopub.execute_input":"2024-06-01T22:05:44.627500Z","iopub.status.idle":"2024-06-01T22:05:44.762452Z","shell.execute_reply.started":"2024-06-01T22:05:44.627469Z","shell.execute_reply":"2024-06-01T22:05:44.761539Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d24c1eb211c0420287ce33067ea51ff7"}},"metadata":{}}]},{"cell_type":"code","source":"val_match= validation_set.map(preprocess_function, batched=True, remove_columns=[\"promptID\", \"pairID\", \"premise\", \"premise_binary_parse\", \"premise_parse\", \"hypothesis\", \"hypothesis_binary_parse\", \"hypothesis_parse\", \"genre\", \"prompt\"]).rename_column('label', 'labels')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:05:45.903906Z","iopub.execute_input":"2024-06-01T22:05:45.904882Z","iopub.status.idle":"2024-06-01T22:05:46.212047Z","shell.execute_reply.started":"2024-06-01T22:05:45.904847Z","shell.execute_reply":"2024-06-01T22:05:46.211052Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424bfeb327fb41af9fbfca51956ce17b"}},"metadata":{}}]},{"cell_type":"code","source":"val_match","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:05:46.913461Z","iopub.execute_input":"2024-06-01T22:05:46.913859Z","iopub.status.idle":"2024-06-01T22:05:46.920465Z","shell.execute_reply.started":"2024-06-01T22:05:46.913826Z","shell.execute_reply":"2024-06-01T22:05:46.919324Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 490\n})"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:05:47.679408Z","iopub.execute_input":"2024-06-01T22:05:47.679795Z","iopub.status.idle":"2024-06-01T22:05:47.684785Z","shell.execute_reply.started":"2024-06-01T22:05:47.679767Z","shell.execute_reply":"2024-06-01T22:05:47.683603Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"val_loader = DataLoader(val_match, batch_size=4, collate_fn=data_collator)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:05:49.160968Z","iopub.execute_input":"2024-06-01T22:05:49.161713Z","iopub.status.idle":"2024-06-01T22:05:49.166242Z","shell.execute_reply.started":"2024-06-01T22:05:49.161680Z","shell.execute_reply":"2024-06-01T22:05:49.165119Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n\n        all_predictions.extend(predictions.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:05:50.004563Z","iopub.execute_input":"2024-06-01T22:05:50.005339Z","iopub.status.idle":"2024-06-01T22:08:11.860734Z","shell.execute_reply.started":"2024-06-01T22:05:50.005307Z","shell.execute_reply":"2024-06-01T22:08:11.859890Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:08:29.905101Z","iopub.execute_input":"2024-06-01T22:08:29.905470Z","iopub.status.idle":"2024-06-01T22:08:29.912950Z","shell.execute_reply.started":"2024-06-01T22:08:29.905443Z","shell.execute_reply":"2024-06-01T22:08:29.912083Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(all_labels, all_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-01T22:08:30.519889Z","iopub.execute_input":"2024-06-01T22:08:30.520822Z","iopub.status.idle":"2024-06-01T22:08:30.538432Z","shell.execute_reply.started":"2024-06-01T22:08:30.520783Z","shell.execute_reply":"2024-06-01T22:08:30.537590Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Accuracy: 0.3469\nPrecision: 0.1204\nRecall: 0.3469\nF1 Score: 0.1787\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}